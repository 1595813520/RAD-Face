Text-to-image diffusion models have demonstrated remarkable potential in high-quality face synthesis. However, existing approaches often struggle to balance fine-grained detail restoration and identity consistency when faced with low-quality or blurred single-image inputs. Moreover, training models for identity-specific generation typically incurs high computational costs. To address these challenges, we propose RAD-Face, a novel Retrieval-Augmented Diffusion framework that significantly enhances the generation quality of low-quality facial inputs by leveraging a large-scale offline face feature database. RAD-Face introduces a tri-branch conditional design into the cross-attention mechanism of the U-Net backbone, incorporating features from the degraded input image, frozen ArcFace identity embeddings, and high-fidelity CLIP and identity features retrieved from the database. By employing dynamic feature fusion and a multimodal regularization strategy, RAD-Face achieves efficient identity preservation and detail reconstruction in single-shot scenarios. Extensive experiments on the FFHQ and CelebA-HQ datasets demonstrate that RAD-Face outperforms state-of-the-art methods across several metrics, including identity consistency, FID, and LPIPS, while reducing inference computational cost to approximately 70% of the baseline. To the best of our knowledge, this is the first work to integrate retrieval-augmented generation (RAG) with diffusion models for face synthesis. Our framework offers an efficient paradigm for identity-sensitive generation tasks, where the incorporation of external knowledge bases effectively mitigates the loss of detail from low-quality inputs. Code and models will be released to facilitate future research.


